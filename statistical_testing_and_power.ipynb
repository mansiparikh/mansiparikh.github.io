{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb0b963-02dd-4b4b-a97b-aeebf7d6006d",
   "metadata": {},
   "source": [
    "# Statistical Testing and Power\n",
    "\n",
    "This notebook goes through some demonstrations of how we perform statistical\n",
    "testing and how we perform power analyses. It focuses on using Monte Carlo\n",
    "simulations to demonstrate the concepts instead of using theoretical\n",
    "statistical distributions for statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f0c41-224f-413d-8095-0beedacfe2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as p9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ced5a-6c4e-4304-a42d-8bf1e136b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56f4ad-0ddf-4453-a849-2118c312b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say that we are getting samples from two Poisson distributions.\n",
    "# As an example, this could be a case where we have divided up 200 cities\n",
    "# into test and control groups and are looking at the number of orders\n",
    "# they got in each city on a particular day. We'll say that the average\n",
    "# city got 5 orders, regardless of whether it was in the test or\n",
    "# control group (i.e., there's no effect of our test). If you want to see what\n",
    "# happens when there is really a difference between groups, change the \n",
    "# alt_mean to be different from the true_mean (e.g., alt_mean = 5.5).\n",
    "n = 100\n",
    "true_mean = 5\n",
    "alt_mean = 5\n",
    "y0 = np.random.poisson(true_mean, size=(n, 1))\n",
    "y1 = np.random.poisson(alt_mean, size=(n, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5b492-4394-4ecd-a136-9a766b25d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need some statistic that summarizes how far apart the samples are\n",
    "# from each other. We could use something like the difference in means\n",
    "# between the groups or the ratio of the means. We'll use the ratio.\n",
    "t = np.mean(y1) / np.mean(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722ce91-278e-4cef-a0b0-3d460b2f9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This statistic summarizes how far apart our samples are, but is this\n",
    "# value in the reasonable range of what we'd expect from ordinary random \n",
    "# variation?\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e23bf-7803-413a-9e87-9b36e83faae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume our null hypothesis (no differenc between groups) is true\n",
    "# and see what the distribution of that statistic (ratio of the means)\n",
    "# looks like. To do that, we'll generate a bunch of hypothetical outcomes\n",
    "# of the experiment under the null hypothesis and record the statistic\n",
    "# generated by each one.\n",
    "n_samples = 10000\n",
    "t_samples = []\n",
    "for sample in range(n_samples):\n",
    "    y0_sample = np.random.poisson(np.mean(list(y0) + list(y1)), size=(n, 1))\n",
    "    y1_sample = np.random.poisson(np.mean(list(y0) + list(y1)), size=(n, 1))\n",
    "    t_sample = np.mean(y1_sample) / np.mean(y0_sample)\n",
    "    t_samples.append(t_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8398ba7-389e-4fa5-86be-f7c82f0a7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the distribution of the statistic. We can compare\n",
    "# our statistic (t) to this distribution to see how unusual it is.\n",
    "plot_out = (\n",
    "    p9.ggplot(pd.DataFrame({'samples': t_samples}))\n",
    "    + p9.aes(x='samples')\n",
    "    + p9.geom_histogram()\n",
    ")\n",
    "plot_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05a8c3-4556-4ad9-9949-00f32ea6ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-value = probability of getting a value as extreme or more extreme than what I got (two-sided).\n",
    "# If t is less than 1, I need the left tail plus the right tail for 1 / t. If\n",
    "# t is greater than 1, I need the right tail plus the left tail for 1 / t.\n",
    "if t < 1:\n",
    "    print((np.sum(t_samples < t) + np.sum(t_samples > (1 / t))) / n_samples)\n",
    "else:\n",
    "    print((np.sum(t_samples > t) + np.sum(t_samples < (1 / t))) / n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82e45b-9e4f-4099-824d-5d465259c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about one-sided vs. two-sided tests?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819bc342-1d78-4a7d-b02e-374cb23d7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does this relate to power analyses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b26870-9f76-40d9-b435-58968a1088e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:temp] *",
   "language": "python",
   "name": "conda-env-temp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
